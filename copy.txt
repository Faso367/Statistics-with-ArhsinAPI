import requests
import zipfile
import os
from tqdm import tqdm  # Для отображения прогресса загрузки
import pandas as pd
from sqlalchemy import create_engine, text
import requests, re
import logging
#from path import abspath
from os import path
#from path import abspath()
from sqlalchemy import Column, Integer, BigInteger, VARCHAR, Boolean, SmallInteger, Date, create_engine, Table, MetaData
from sqlalchemy.schema import ForeignKey
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy.dialects.postgresql import TSVECTOR
import psycopg2 
import hashlib
import zipfile
from pathlib import Path
import time

Base = declarative_base()
metadata = MetaData()
# ----------------------- Модели базы данных

class UniquePoveritelOrgs(Base):
	__tablename__ = 'UniquePoveritelOrgs'
	id = Column(BigInteger(), primary_key=True)
	poveritelOrg = Column(VARCHAR(256), unique=True)


class UniqueTypeNames(Base):
    __tablename__ = 'UniqueTypeNames'
    id = Column(BigInteger(), primary_key=True)
    typeName = Column(VARCHAR(512))
    typeName_tsvector = Column(TSVECTOR())

class UniqueRegisterNumbers(Base):
	__tablename__ = 'UniqueRegisterNumbers'
	id = Column(BigInteger(), primary_key=True)
	registerNumber = Column(VARCHAR(16), unique=True)

class UniqueTypes(Base):
    __tablename__ = 'UniqueTypes'
    id = Column(BigInteger(), primary_key=True)
    type = Column(VARCHAR(512))
    type_tsvector = Column(TSVECTOR())
     
     
class UniqueModifications(Base):
    __tablename__ = 'UniqueModifications'
    id = Column(BigInteger(), primary_key=True)
    modification = Column(VARCHAR(512))
    modification_tsvector = Column(TSVECTOR())


class EquipmentInfoPartitioned(Base):
    __tablename__ = 'EquipmentInfoPartitioned'
    id = Column(BigInteger(), primary_key=True)
    #registerNumber = Column(VARCHAR(16))
    serialNumber = Column(VARCHAR(256))
    svidetelstvoNumber = Column(VARCHAR(256))
    poverkaDate = Column(Date())
    konecDate = Column(Date())
    vri_id = Column(BigInteger())
    isPrigodno = Column(Boolean())
    poveritelOrgId = Column(Integer(), ForeignKey('UniquePoveritelOrgs.id'))
    typeNameId = Column(Integer(), ForeignKey('UniqueTypeNames.id'))
    registerNumberId = Column(Integer(), ForeignKey('UniqueRegisterNumbers.id'))
    typeId = Column(Integer(), ForeignKey('UniqueTypes.id'))
    modificationId = Column(Integer(), ForeignKey('UniqueModifications.id'))
    year = Column(SmallInteger())


class DownloadedFiles(Base):
    __tablename__ = 'DownloadedFiles'
    id = Column(BigInteger, primary_key=True)
    fileId = Column(BigInteger)
    fileName = Column(VARCHAR(256))


temp_table = Table('temp_table', metadata,
    #Column('registerNumber', VARCHAR(16)),
    Column('serialNumber', VARCHAR(256)),
    Column('svidetelstvoNumber', VARCHAR(256)),
    Column('poverkaDate', Date()),
    Column('poveritelOrg', VARCHAR(256)),
    Column('typeName', VARCHAR(512)),
    Column('registerNumber', VARCHAR(16)),
    Column('type', VARCHAR(512)),
    Column('modification', VARCHAR(512)),
    Column('konecDate', Date()),
    Column('vri_id', BigInteger()),
    Column('isPrigodno', Boolean()),
    # Column('poveritelOrgId', Integer(), ForeignKey('UniquePoveritelOrgs.id')),
    # Column('typeNameId', Integer(), ForeignKey('UniqueTypeNames.id')),
    # Column('registerNumberId', Integer(), ForeignKey('UniqueRegisterNumbers.id')),
    # Column('typeId', Integer(), ForeignKey('UniqueTypes.id')),
    # Column('modificationId', Integer(), ForeignKey('UniqueModifications.id')),
    Column('year', SmallInteger)
)


class ArshinDataDownloader:
    '''Класс отвечает за скачивание, распаковку и загрузку в БД инф-ии с сайта ФГИС Аршин'''

    # Фильтр, исключающий предупреждения
    class NoWarningFilter(logging.Filter):
        def filter(self, record):
            return record.levelno not in (pd.errors.DtypeWarning,)

    def __init__(self):
        self.__startUrl = 'https://fgis.gost.ru/fundmetrology/exporter/poverki/'
        #self.__current_year =  datetime.now().year
        self.__engine = create_engine('postgresql://postgres:password@localhost:5432/Arshindb')
        self.__Session = sessionmaker(bind=self.__engine)
        self.__session = self.__Session()

        # Настройка логгера
        self.logger = logging.getLogger('arshin_logger')
        self.logger.setLevel(logging.DEBUG)  # Установите уровень логирования на DEBUG
        
        # Определение пути к файлу логирования
        script_directory = os.path.dirname(os.path.abspath(__file__))
        log_file_path = os.path.join(script_directory, 'arshinDataDownloader.log')
        
        # Создание обработчика для записи в файл
        self.file_handler = logging.FileHandler(log_file_path)
        self.file_handler.setLevel(logging.DEBUG)

        # Создание обработчика для вывода на консоль
        self.console_handler = logging.StreamHandler()
        self.console_handler.setLevel(logging.DEBUG)

        # Установка формата для обработчиков
        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        self.file_handler.setFormatter(formatter)
        self.console_handler.setFormatter(formatter)

        # Добавляем фильтр к обработчикам
        self.console_handler.addFilter(ArshinDataDownloader.NoWarningFilter())
        self.file_handler.addFilter(ArshinDataDownloader.NoWarningFilter())

        # Добавление обработчиков к логгеру
        self.logger.addHandler(self.file_handler)
        self.logger.addHandler(self.console_handler)


    def __get_new_file_names_and_identifiers(self, url):
        '''Возвращает списки наименований файлов и их идентификаторов, которые надо закачать (исключает уже скачанные файлы)'''

        # Запрос для получения содержимого файла
        response = requests.get(url + 'manifest.json')

        # Получаем список ранее скачанных файлов и их идентификаторов из БД 
        oldColumn = self.__session.query(DownloadedFiles.fileId, DownloadedFiles.fileName).all()
        
        oldIdentifiers = set([str(row[0]) for row in oldColumn])
        oldFileNames = set([row[1] for row in oldColumn])
        currentIdentifiers = []

        self.logger.info(f"Статус код ответа от Аршин: {response.status_code}")

        # Проверка успешности запроса
        if response.status_code == 200:

            # Преобразование содержимого в JSON
            data = response.json()

            # Получение значений поля "relativeUri"
            currentIdentifiers = [delta['id'] for delta in data['deltas']]
            currentFileNames = [delta['relativeUri'] for delta in data['deltas']]

            newIdentifiers = list(set(currentIdentifiers).difference(oldIdentifiers))
            newFileNames = list(set(currentFileNames).difference(oldFileNames))

            # Проверка, что списки одинаковой длины
            if len(newIdentifiers) != len(newFileNames):
                self.logger.error("Ошибка чтения manifest.json, списки должны быть одинаковой длины ", exc_info=True)
                raise ValueError("Списки данных должны быть одинаковой длины")

        else:
            self.logger.error(f'Не удалось получить файл: {response.status_code}')
            self.__session.close()
        return newFileNames, newIdentifiers


    def __download_file(self, url, local_filename):
        '''Скачивает файл по удалённому пути'''
        #try:
        #with requests.get(url, stream=True, timeout=2) as response: !!!!! можно делать перерыв между запросами
        with requests.get(url, stream=True) as response:
            response.raise_for_status()
            with open(local_filename, 'wb') as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)
        self.logger.info(f'Был скачан файл {local_filename.split('/')[-1]}')
        #except requests.exceptions.RequestException as e:
            #print("Ошибка при скачивании файла:", e)
            #self.logger.error("Ошибка при скачивании файла:", e)

    # def __download_huge_file(self, url, local_filename):
    #     chunk_size = 1024  # загружаем по кусочкам
    #     offset = os.path.getsize(local_filename) if os.path.exists(local_filename) else 0  # можем продолжить в другой раз

    #     headers = {'Range': f'bytes={offset}-'}
    #     response = requests.get(url, headers=headers, stream=True)

    #     # Продолжаем загрузку с места остановки
    #     with open(local_filename, 'ab') as f:  # 'ab' – позволяет продолжать запись в файл в бинарном режиме с места остановки
    #         f.seek(offset)
    #         for chunk in response.iter_content(chunk_size=chunk_size):
    #             if chunk:
    #                 f.write(chunk)

    # def download_file_with_retry(self, url, local_filename, max_retries=5, retry_delay=5):
    #     chunk_size = 1024  # загружаем по кусочкам
    #     offset = os.path.getsize(local_filename) if os.path.exists(local_filename) else 0  # можем продолжить в другой раз
    #     headers = {'Range': f'bytes={offset}-'}

    #     for attempt in range(max_retries):
    #         try:
    #             response = requests.get(url, headers=headers, stream=True, timeout=retry_delay)
    #             response.raise_for_status()  # поднять исключение для кода статуса HTTP 4xx/5xx

    #             # Продолжаем загрузку с места остановки
    #             with open(local_filename, 'ab') as f:  # 'ab' – позволяет продолжать запись в файл в бинарном режиме с места остановки
    #                 f.seek(offset)
    #                 for chunk in response.iter_content(chunk_size=chunk_size):
    #                     if chunk:
    #                         print(chunk)
    #                         #f.write(chunk)
                
    #             print("Download completed successfully")
    #             return True

    #         except requests.exceptions.RequestException as e:
    #             print(f"Error occurred: {e}. Retrying in {retry_delay} seconds...")
    #             #time.sleep(retry_delay)
    #             offset = os.path.getsize(local_filename)  # обновить смещение
    #             print(offset)

    #     print("Max retries reached. Download failed.")
    #     return False

    def download_file_with_retry(self, url, local_filename, max_retries=10, retry_delay=10, chunk_size=1024):
        # Проверяем наличие файла и его размер
        offset = os.path.getsize(local_filename) if os.path.exists(local_filename) else 0

        for attempt in range(max_retries):
            try:
                headers = {'Range': f'bytes={offset}-'}
                response = requests.get(url, headers=headers, stream=True, timeout=retry_delay)
                response.raise_for_status()  # Поднять исключение для кода статуса HTTP 4xx/5xx
                
                total_size = int(response.headers.get('content-length', 0)) + offset
                print(f"Starting download from {offset} to {total_size}")

                with open(local_filename, 'ab') as f:
                    pbar = tqdm(total=total_size, initial=offset, unit='B', unit_scale=True, desc=local_filename)
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))
                    pbar.close()
                
                # Проверка, завершена ли загрузка
                if os.path.getsize(local_filename) >= total_size:
                    print("Download completed successfully")
                    return True
                else:
                    print(f"Download incomplete, retrying from {os.path.getsize(local_filename)}...")
                    offset = os.path.getsize(local_filename)  # обновить смещение

            except requests.exceptions.RequestException as e:
                print(f"Error occurred: {e}. Retrying in {retry_delay} seconds...")
                #time.sleep(retry_delay)
                offset = os.path.getsize(local_filename)  # обновить смещение

        print("Max retries reached. Download failed.")
        return False



    def __unzip_file(self, zip_filename, extract_to):
        '''Распаковывает zip архив'''
        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
            # Удаляем распакованный архив
        os.remove(zip_filename)

    def __add_file_to_db(self, path):
        '''Читает содержимое файла, валидирует и записывает в БД. Запись ведётся в 3 таблицы.'''

        firstTime = True

        names = ['Number', 'poveritelOrg', 'registerNumber', 'typeName', 'type', 'modification',
                     'serialNumber', 'poverkaDate', 'konecDate', 'svidetelstvoNumber', 'isPrigodno',
                     'Date1', 'Date2', 'Pusto', 'vri_id', 'rabbish']
        df = pd.read_csv(path, chunksize = 50000, on_bad_lines='skip', delimiter=';',quotechar='"', header=None, names=names)#, header = 0)

        for chunk in df:
            
            chunk = chunk.drop(columns=['Number', 'Date1', 'Date2', 'Pusto', 'rabbish'])
            search = lambda x: x if re.search(r"\d{4}-\d{2}-\d{2}", str(x)) else 'not found'

            chunk['poverkaDate'] = chunk['poverkaDate'].map(search)
            chunk['konecDate'] = chunk['konecDate'].map(search)
            chunk = chunk.drop(chunk.query('poverkaDate == "not found"').index)
            chunk = chunk.drop(chunk.query('konecDate == "not found"').index)

            allColumns = ['poveritelOrg', 'registerNumber', 'typeName', 'type', 'modification',
            'serialNumber', 'poverkaDate', 'konecDate', 'svidetelstvoNumber', 'isPrigodno', 'vri_id']

            chunk[allColumns].replace('  ', ' ')
            chunk['poverkaDate'] = pd.to_datetime(chunk['poverkaDate'], format='%Y-%m-%d', errors='coerce')
            chunk['konecDate'] = pd.to_datetime(chunk['konecDate'], format='%Y-%m-%d', errors='coerce')
            chunk['isPrigodno'] = chunk['isPrigodno'].astype('bool')
            chunk['vri_id'] = pd.to_numeric(chunk['vri_id'], errors='coerce').astype('Int64')
            chunk = chunk.where(pd.notnull(chunk), None) # Новый код

            # Будем добавлять данные в соответствующие колонки этих двух таблиц 
            table_column = {'UniquePoveritelOrgs' : 'poveritelOrg', 'UniqueTypeNames' : 'typeName', 'UniqueRegisterNumbers' : 'registerNumber', 'UniqueTypes' : 'type', 'UniqueModifications' : 'modification'}

            for table, col in table_column.items():
                old = pd.read_sql_table(table, self.__engine)[col].to_frame()
                new = chunk[col].to_frame()

                # Определяем отсутствующие строки
                merged_data = new.merge(old, on=col, how='left', indicator=True)
                missing_data = merged_data[merged_data['_merge'] == 'left_only'].drop(columns=['_merge'])
                unique_missing_data = missing_data.drop_duplicates(subset=col)

                if not unique_missing_data.empty:
                    #print('Было добавлено ' + str(len(unique_missing_data)) + ' уникальных строк(-и)')
                    self.logger.info(f'Было добавлено {len(unique_missing_data)} уникальных(-е) строк(-и)')
                    # Добавляем в таблицы наименования новых организаций и типов устройств 
                    unique_missing_data.to_sql(name=table, con=self.__engine, if_exists='append', index=False)

            # # Типы данных для временной таблицы
            # dtype2 = {
            # #'registerNumber': VARCHAR(16),
            # 'serialNumber': VARCHAR(256),
            # 'svidetelstvoNumber': VARCHAR(256),
            # 'poverkaDate': Date,
            # 'konecDate': Date,
            # 'vri_id': BigInteger,
            # 'isPrigodno': Boolean,
            # 'poveritelOrgId': Integer,
            # 'typeNameId': Integer,
            # 'registerNumberId' : Integer,
            # 'typeId' : Integer,
            # 'modificationId' : Integer,
            # 'year': SmallInteger
            # }

            # Типы данных для временной таблицы
            dtype2 = {
            'serialNumber': VARCHAR(256),
            'svidetelstvoNumber': VARCHAR(256),
            'registerNumber': VARCHAR(16),
            'typeName': VARCHAR(512),
            'type': VARCHAR(512),
            'modification': VARCHAR(512),
            'poveritelOrg': VARCHAR(256), # ???????????????
            'poverkaDate': Date,
            'konecDate': Date,
            'vri_id': BigInteger,
            'isPrigodno': Boolean,
            'year': SmallInteger
            }


            # Создаём из куска временную таблицу в оперативной памяти, чтобы затем её добавить в БД
            #chunk.to_sql('temp_table', self.__engine, if_exists='replace', index=False, dtype=dtype2)
            if firstTime:
                chunk.to_sql('temp_table', self.__engine, if_exists='append', index=False, dtype=dtype2)
                firstTime = False
            else:
                chunk.to_sql('temp_table', self.__engine, if_exists='replace', index=False, dtype=dtype2)

            metadata = MetaData()
            metadata.reflect(bind=self.__engine, only=['temp_table'])
            #print(777)
            # stmt = text("""INSERT INTO "EquipmentInfoPartitioned" ("vri_id", "serialNumber", "poverkaDate", "konecDate", "svidetelstvoNumber", "isPrigodno", "year", "poveritelOrgId", "typeNameId", "registerNumberId")
            #     SELECT "vri_id", "serialNumber", "poverkaDate", "konecDate", "svidetelstvoNumber", "isPrigodno", EXTRACT(YEAR FROM "poverkaDate"), "UniquePoveritelOrgs"."id", "UniqueTypeNames"."id", "UniqueRegisterNumbers"."id"
            #     FROM "temp_table"
            #     JOIN "UniquePoveritelOrgs" ON "temp_table"."poveritelOrg" = "UniquePoveritelOrgs"."poveritelOrg"
            #     JOIN "UniqueTypeNames" ON "temp_table"."typeName" = "UniqueTypeNames"."typeName"
            #     JOIN "UniqueRegisterNumbers" ON "temp_table"."registerNumber" = "UniqueRegisterNumbers"."registerNumber"
            #     WHERE "poverkaDate" IS NOT NULL AND EXTRACT(YEAR FROM "poverkaDate") > 2018;
            #     """)

        #deleteStmt = text("""delete from "EquipmentInfoPartitioned" where "svidetelstvoNumber" IN(SELECT "svidetelstvoNumber" from "temp_table");""")

        stmt = text("""INSERT INTO "EquipmentInfoPartitioned" ("vri_id", "serialNumber", "poverkaDate", "konecDate", "svidetelstvoNumber", "isPrigodno", "year",
                     "poveritelOrgId", "typeNameId", "registerNumberId", "typeId", "modificationId")
                SELECT "vri_id", "serialNumber", "poverkaDate", "konecDate", "svidetelstvoNumber", "isPrigodno", EXTRACT(YEAR FROM "poverkaDate"),
                     "UniquePoveritelOrgs"."id", "UniqueTypeNames"."id", "UniqueRegisterNumbers"."id", "UniqueTypes"."id", "UniqueModifications"."id"
                FROM "temp_table"
                JOIN "UniquePoveritelOrgs" ON "temp_table"."poveritelOrg" = "UniquePoveritelOrgs"."poveritelOrg"
                JOIN "UniqueTypeNames" ON "temp_table"."typeName" = "UniqueTypeNames"."typeName"
                JOIN "UniqueRegisterNumbers" ON "temp_table"."registerNumber" = "UniqueRegisterNumbers"."registerNumber"
                JOIN "UniqueTypes" ON "temp_table"."type" = "UniqueTypes"."type"
                JOIN "UniqueModifications" ON "temp_table"."modification" = "UniqueModifications"."modification"
                WHERE "poverkaDate" IS NOT NULL AND EXTRACT(YEAR FROM "poverkaDate") > 2018;
                """)
                            #ON CONFLICT ("svidetelstvoNumber") DO UPDATE SET vri_id = EXCLUDED.vri_id;
        #self.__session.execute(deleteStmt)
        #self.__session.commit()
        self.__session.execute(stmt)
        self.__session.commit()
        #self.__session.close()

        self.logger.info("Файл добавлен в БД")

        

    # def __verify_zip_file(file_path):
    #     # Проверяем целостность ZIP файла
    #     try:
    #         with zipfile.ZipFile(file_path, 'r') as zip_ref:
    #             bad_file = zip_ref.testzip()
    #             if bad_file is not None:
    #                 print(f"Corrupt file found in ZIP: {bad_file}")
    #                 return False
    #         return True
    #     except zipfile.BadZipFile:
    #         print("The file is not a zip file or it is corrupted")
    #         return False

    def calculate_md5(self, file_path):
        # Вычисляем хэш-сумму MD5 файла
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()



    def is_folder_empty(self, folder_path):
        folder = Path(folder_path)
        # Проверяем, содержит ли папка какие-либо файлы или подпапки
        return not any(folder.iterdir())

    def extract_and_add_to_db_old_files(self, rootPath):
        #print(rootPath)
        # Если эта папка содержит подпапки или файлы
        
        # Проверяем, содержит ли папка какие-либо файлы или подпапки
        #return not any(folder.iterdir())
        """ Разархивирует zip файл и его содержимое, включая вложенные zip файлы. Удаляет zip файл(ы) после извлечения"""
        for z, dirs, files in os.walk(rootPath):
            if not self.is_folder_empty(z):
                for filename in files:
                    fileSpec = os.path.join(z, filename)
                    if filename[-4:] == '.zip':
                        if 'snapshot' in fileSpec.split('\\')[-1]:
                            self.__unzip_file(os.path.join(z, filename), self.rootPath)
                            self.extract_and_add_to_db_old_files(self.rootPath)
                        self.__unzip_file(fileSpec, z)
                        #self.__add_file_to_db(os.path.join(root, filename[:-4]))
                        print(f'Файл {filename[:-4]} добавлен в БД')
                        os.remove(os.path.join(z, filename[:-4]))
                        #os.remove(fileSpec)
                    elif filename[-4:] == '.csv':
                        #self.__add_file_to_db(fileSpec)
                        print(f'Файл {filename} добавлен в БД')
                        os.remove(fileSpec)
                    else:
                        print(111)
                    #os.remove(fileSpec)
                    #self.extract_and_add_to_db_old_files(fileSpec, root)
                    print(fileSpec.split('\\')[-1])
                    #if 'snapshot' in fileSpec.split('\\')[-1]:
                    #    os.removedirs(self.rootPath + '\\usr')
                        #self.extract_and_add_to_db_old_files()
                    #else:
                    self.extract_and_add_to_db_old_files(z)
            else:
                #os.removedirs(z)
                os.rmdir(z)
                #self.extract_and_add_to_db_old_files(z)

    def __add_old_data(self):
        # Запрос для получения содержимого файла
        '''
        response = requests.get(self.__startUrl + 'manifest.json')

        self.logger.info(f"Статус код ответа от Аршин: {response.status_code}")

        # Проверка успешности запроса
        #if response.status_code == 200:

        # Преобразование содержимого в JSON
        data = response.json()

        newFileName = data['snapshots'][-1]['relativeUri']
        correctFileHashSum = data['snapshots'][-1]['md5sum']
        '''
        newFileName = 'poverki.snapshot.20240801.csv.zip'


        # startAbsUrl = os.path.join(os.path.abspath(newFileName)).replace(newFileName, '').replace("\\", '/')
        startAbsUrl = os.path.join(os.path.abspath(newFileName)).replace(newFileName, '')
        downloadedZipPath = startAbsUrl + newFileName
        #self.download_file_with_retry(self.__startUrl + newFileName, downloadedZipPath)
        #downloadedZipPath = 'C:\Users\LIKORIS001\Desktop\NewCreateScript\poverki.snapshot.20240801.csv\usr\exporter\public\poverki.delta.20240704--20240705.csv.zip'
        downloadedFolderPath = downloadedZipPath[:-4]

        self.__unzip_file(downloadedZipPath, downloadedFolderPath) #В первый раз !!!
        
        self.rootPath = downloadedFolderPath
        self.extract_and_add_to_db_old_files(self.rootPath)
        '''
        # Вычисляем хэш-сумму MD5 для проверки целостности (если известна хэш-сумма)
        downloaded_md5 = self.calculate_md5(zipPath)
        if downloaded_md5 == correctFileHashSum:
            print("File downloaded correctly")

            filePath = zipPath[:-4]
            self.__unzip_file(zipPath, filePath)
            #filePath + '/' + newFileName[:-4]
    
            for lists in os.walk(filePath):
                for list in lists:
                    if list[:2] == 'C:' and list[-6:]:
                        path = list
                    else:
                        for item in list:
                            if item[-4:] == '.csv':
                                print(item)
                                # with warnings.catch_warnings():
                                # warnings.filterwarnings('ignore')
                                filePath = path + "\\" + item
        else:
            print("File is corrupt or has been altered")
            return 0'''
        #insideFolderFileNames = os.walk(downloadedFolderPath)
        '''
        while len(os.walk(downloadedFolderPath)) > 0:
            for lists in os.walk(downloadedFolderPath):
                for list in lists:
                    #if list[:2] == 'C:' and list[-6:]:
                    if list[:2] == 'C:':
                        pathBeforeFile = list
                    else:
                        for item in list:
                            if item[-4:] == '.zip':
                                PathToFile = pathBeforeFile + "\\" + item
                                #filePath = path + "/" + item
                                self.__unzip_file(PathToFile, pathBeforeFile)
                                unzippedFilePath = pathBeforeFile + "\\" + item[:-4]
                                self.__add_file_to_db(unzippedFilePath)
                                #os.remove(unzippedFilePath)
                                #self.__add_file_to_db(filePath + '/' + newFileName[:-4])

                                # Удаляем файл
                                #os.remove(filePath + '/' + newFileName[:-4])
                                # Удаляем пустую папку
                                #os.rmdir(filePath)
                                print(f'файл {newFileName[:-4]} был добавлен')
                                #break
                            # if item[-4:] == '.csv':
                            #     print(item)
                            #     # with warnings.catch_warnings():
                            #     # warnings.filterwarnings('ignore')
                            #     filePath = path + "\\" + item
        '''
        print(888)


    def Main(self):

        #downloadOldData = int(input('Если хотите скачать даные за прошлые месяца, введите 1, иначе 0: '))
        downloadOldData = 1
        if downloadOldData == 1:
            self.__add_old_data()
        else:
            res = self.__get_new_file_names_and_identifiers(self.__startUrl)
            newFileNames = res[0]
            newIdentifiers = res[1]
            countFiles = 0

            for newIdentifier, newFileName in zip(newIdentifiers, newFileNames):
                startAbsUrl = os.path.join(path.abspath(newFileName)).replace(newFileName, '').replace("\\", '/')
                zipPath = startAbsUrl + newFileName
                self.__download_huge_file(self.__startUrl + newFileName, zipPath)
                #self.__download_file(self.__startUrl + newFileName, zipPath)
                filePath = zipPath[:-4]
                self.__unzip_file(zipPath, filePath)
                self.__add_file_to_db(filePath + '/' + newFileName[:-4])
                new_record = DownloadedFiles(fileId=newIdentifier, fileName=newFileName)
                self.__session.add(new_record)
                self.__session.commit()
                # Удаляем файл
                os.remove(filePath + '/' + newFileName[:-4])
                # Удаляем пустую папку
                os.rmdir(filePath)
                countFiles += 1
                if countFiles == 2:
                    break # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            self.__session.close()
            self.logger.info(f"Количество добавленных файлов = {countFiles}")
ekz = ArshinDataDownloader()
ekz.Main()



                            
                            #if item.count('poverki.delta.20230702--20230703') > 0:
                            # with warnings.catch_warnings():
                            #     warnings.filterwarnings('ignore')
                            #     filePath = path + "\\" + item

                            #     columns = ['Number', 'poveritelOrg', 'registerNumber', 'typeName', 'typeNumber1', 'typeNumber2',
                            #         'serialNumber', 'poverkaDate', 'konecDate', 'svidetelstvoNumber', 'isPrigodno',
                            #         'Date1', 'Date2', 'Pusto', 'vri_id', 'rabbish']
                            #     df = pd.read_csv(filePath,chunksize = 1000000, on_bad_lines='skip', delimiter=';', header=None, names=columns)#, header = 0)

                            #     for chunk in df:
                            #         #print(chunk)
                            #         chunk = chunk.drop(columns=['Number', 'typeNumber1', 'typeNumber2', 'Date1', 'Date2', 'Pusto', 'rabbish'])
                            #         #chunk = chunk.loc[chunk['isPrigodno'] != 0]
                            #         #print('----------------------------------')
                            #         #print(chunk)
                            #         chunk.to_csv('C:\\Users\\LIKORIS001\\Desktop\\CSVReaderPython\\allDataNew2.csv', sep=';')

                            #     print('Файл ' + item + ' был записан')









def __add_ResultValid(self, path):
    '''Читает содержимое валидированного файла и записывает в партицированную таблицу и 3 других уникальных. Запись ведётся в 4 таблицы.'''

    names = ['poveritelOrg', 'registerNumber', 'typeName', 'serialNumber',
                'poverkaDate', 'konecDate', 'svidetelstvoNumber', 'isPrigodno', 'vri_id']
    df = pd.read_csv(path, chunksize = 80000, on_bad_lines='skip', delimiter=';',quotechar='"', header=None, names=names)#, header = 0)

    for chunk in df:
        # Будем добавлять данные в соответствующие колонки этих двух таблиц 
        table_column = {'UniquePoveritelOrgs' : 'poveritelOrg', 'UniqueTypeNames' : 'typeName', 'UniqueRegisterNumbers' : 'registerNumber'}

        for table, col in table_column.items():
            old = pd.read_sql_table(table, self.__engine)[col].to_frame()
            new = chunk[col].to_frame()

            # Определяем отсутствующие строки
            merged_data = new.merge(old, on=col, how='left', indicator=True)
            missing_data = merged_data[merged_data['_merge'] == 'left_only'].drop(columns=['_merge'])
            unique_missing_data = missing_data.drop_duplicates(subset=col)

            if not unique_missing_data.empty:
                #print('Было добавлено ' + str(len(unique_missing_data)) + ' уникальных строк(-и)')
                self.logger.info(f'Было добавлено {len(unique_missing_data)} уникальных(-е) строк(-и)')
                # Добавляем в таблицы наименования новых организаций и типов устройств 
                unique_missing_data.to_sql(name=table, con=self.__engine, if_exists='append', index=False)

        # Типы данных для временной таблицы
        dtype2 = {
        #'registerNumber': VARCHAR(16),
        'serialNumber': VARCHAR(256),
        'svidetelstvoNumber': VARCHAR(256),
        'poverkaDate': Date,
        'konecDate': Date,
        'vri_id': BigInteger,
        'isPrigodno': Boolean,
        'poveritelOrg': Integer,
        'typeName': Integer,
        'registerNumber' : Integer,
        #'year': SmallInteger
        }
        # Создаём из куска временную таблицу в оперативной памяти, чтобы затем её добавить в БД
        chunk.to_sql('temp_table', self.__engine, if_exists='replace', index=False, dtype=dtype2)
        metadata = MetaData()
        metadata.reflect(bind=self.__engine, only=['temp_table'])

        print(chunk)

        stmt = text("""INSERT INTO "EquipmentInfoPartitioned" ("vri_id", "serialNumber", "poverkaDate", "konecDate", "svidetelstvoNumber", "isPrigodno", "year", "poveritelOrgId", "typeNameId", "registerNumberId")
            SELECT "vri_id", "serialNumber", "poverkaDate", "konecDate", "svidetelstvoNumber", "isPrigodno", EXTRACT(YEAR FROM "poverkaDate"), "UniquePoveritelOrgs"."id", "UniqueTypeNames"."id", "UniqueRegisterNumbers"."id"
            FROM "temp_table"
            JOIN "UniquePoveritelOrgs" ON "temp_table"."poveritelOrg" = "UniquePoveritelOrgs"."poveritelOrg"
            JOIN "UniqueTypeNames" ON "temp_table"."typeName" = "UniqueTypeNames"."typeName"
            JOIN "UniqueRegisterNumbers" ON "temp_table"."registerNumber" = "UniqueRegisterNumbers"."registerNumber"
            WHERE "poverkaDate" IS NOT NULL AND EXTRACT(YEAR FROM "poverkaDate") > 2018;
            """)

        self.__session.execute(stmt)
        self.__session.commit()

    self.logger.info("Файл добавлен в БД")